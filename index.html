<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="" />
  <title>MimicDroid</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/css/glide.core.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">

</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h2 class="title is-1 publication-title">MimicDroid: In-Context Learning <br/>for Humanoid Manipulation<br/> by Watching Humans </h2>
            <h2 class="subtitle is-2 publication-subtitle"></h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://shahrutav.github.io/" target="_blank">Rutav Shah</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/harry-wang-16470a211/" target="_blank">Qi Wang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://zhenyujiang.me/" target="_blank">Zhenyu Jiang</a><sup>*</sup>,</span>
                    <span class="author-block">
                    <a href="https://shuijing725.github.io/" target="_blank">Shuijing Liu</a><sup>*</sup>,</span>
                    <span class="author-block">
                  <a href="https://sateeshkumar21.github.io/" target="_blank">Sateesh Kumar</a>,</span>
                  <span class="author-block">
                    <a href="https://mingyoseo.com/" target="_blank"> Mingyo Seo</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://robertomartinmartin.com/" target="_blank">Roberto Martín-Martín</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://yukezhu.me/" target="_blank">Yuke Zhu</a>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">

                    <span class="author-block">The University of Texas at Austin
                      <!-- <br>Conferance name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>
                  <br>
                    <div class="column has-text-centered">
                      <div class="publication-links">
                        <!-- PDF Link. -->
                        <span class="link-block">
                          <a href="."
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>Paper</span>
                          </a>
                        </span>
                        <!--arXiv Link.-->
                        <span class="link-block">
                          <a
                            href="."
                            class="external-link button is-normal is-rounded is-dark"
                          >
                            <span class="icon">
                              <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                          </a>
                        </span>
                        <!-- Video Link. -->
                        <!-- <span class="link-block">
                          <a href="https://www.youtube.com/watch?v=ts0e2HHp3Lk"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-youtube"></i>
                            </span>
                            <span>Video</span>
                          </a>
                        </span> -->
                        <!-- Code Link. -->
                        <span class="link-block">
                          <a href="."
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code (Coming Soon)</span>
                            </a>
                        </span>
                        <!-- Dataset Link. -->
                        <!-- <span class="link-block">
                          <a href="https://drive.google.com/drive/folders/1mNJmnyzIoCudRcTdRVrN3WAiuWIM8355"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="far fa-images"></i>
                            </span>
                            <span>Dataset</span>
                            </a> -->
                      </div>
                    </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop grey-bg">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
     
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">  
            <p>
              We envision humanoids that can efficiently adapt to new tasks, achieving few-shot manipulation in ever-changing environments. In-context learning (ICL) is a promising framework to achieve this goal, but existing ICL approaches typically rely on costly teleoperated datasets or limited expressivity of simulations for training. In this work, we propose using human play videos—continuous, unlabeled videos of people interacting freely with their environment by their curiosity—as a scalable and diverse data source. Learning from human play videos poses two key challenges: (i) the absence of labeled data to train policies to learn in-context; (ii) the embodiment gap between humans and humanoids. We introduce MimicDroid, a method that enables humanoids to perform ICL by using only human play videos as training data. To overcome the lack of supervision, MimicDroid extracts trajectory pairs exhibiting similar manipulation behaviors in a self-supervised manner, e.g., “placing the bread on the oven tray,” and “placing a bagel on the oven tray.” The policy is trained to predict actions of one trajectory, conditioned on others with similar manipulation behaviors. This trains the model to learn in-context from the provided examples. To address the embodiment gap, first, MimicDroid retargets human wrist poses estimated from RGB videos to the humanoid, leveraging their kinematic similarity. Second, to handle the visual gap, it applies random patch masking during training to prevent overfitting to human-specific cues. To systematically evaluate few-shot learning for humanoids, we introduce an open-source simulation benchmark with three levels of increasing generalization difficulty from seen to unseen objects and environments. In both simulation and the real world, MimicDroid outperforms state-of-the-art methods, achieving nearly twofold better generalization to novel objects in the real world.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Video</h2>
      <p class="subtitle is-6">
        Watch the video to see MimicDroid in action!
      </p>

      <video
        controls
        preload="metadata"
        style="max-width:100%; border-radius:8px;"
      >
        <source src="./static/images/summary_video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  </section>



  <!-- Method Overview -->
  <section class="section">
  <div class="container is-max-desktop has-text-centered grey-bg">
    <h2 class="title is-3">Method Overview</h2>
    <figure class="image">
      <img src="./static/images/mew_method_figurev2-1.png" 
           alt="Overview of MIMICDROID" />
          
      <figcaption class="method-caption has-text-justified">
        <strong>Overview of MimicDroid.</strong> MimicDroid performs meta-training for in-context learning (Meta-ICL) by constructing context-target pairs from human play videos.
    For a target segment, we retrieve the top-k most similar trajectory segments (top-left) based on observation-action similarity (top-right) to serve as context.
    These context-target pairs are used to teach the policy in-context learning (bottom-left). To overcome the human-robot visual gap and avoid overfitting to human-specific visual cues, we apply visual masking to input images (bottom-right), improving transferability.
      </figcaption>
    </figure>
  </div>
</section>

  

  <!-- Experiments -->
   <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Quantitative Analaysis</h2>
          <div class="content">
            <h3 class="title is-4" style="margin-top: 1.5em;">Simulation Benchmark Results</h3>
            <figure class="image">
              <img src="./static/images/results.png" alt="experiment results" />
              <figcaption>
               <p>Success rates of MimicDroid in the simulation benchmark, comparing both the Abstract and GR1 embodiments across levels of generalization difficulty (L1–L3). MimicDroid consistently outperforms prior approaches and ablations, demonstrating strong transfer from human play videos to humanoid control.</p>
              </figcaption>
            </figure>
            </div>
            <h3 class="title is-4" style="margin-top: 1.5em;">How well MimicDroid's performance scales with the number of in-context examples?</h3>
            
            <div class="columns is-vcentered grey-bg">
              <div class="column is-half">
                <img src="./static/images/n_prompts-1.png" alt="Number of Prompts Results" style="max-width: 600px; width: 100%;" />
              </div>
              <div class="column is-half">
                <div class="content has-text-justified ">
                  <p><strong></strong> We evaluate how MIMICDROID’s performance varies with the number of
context demonstrations provided at test time. We observe a clear improvement in performance as the number of context examples increases from 1 to 3. However,
the gains drop with 4 to 6 examples due to the limited context length supported by the transformer policy during training. Naively extending context length is computationally
expensive, with memory scaling quadratically in sequence length; future work can explore more efficient ways to handle more in-context examples.</p>
                </div>
              </div>
            </div>

            <h3 class="title is-4" style="margin-top: 1.5em;">How does the quality of training data used in the Meta-ICL affect ICL performance?</h3>

            <div class="columns is-vcentered">
              <div class="column is-half">
                <img src="./static/images/n_topk-1.png" alt="Top-K Results" style="max-width: 600px; width: 100%;" />
              </div>
              <div class="column is-half">
                <div class="content has-text-justified">
                  <p><strong</strong> To evaluate the impact of data quality on Meta-ICL, we vary the number of similar
context segments retrieved during training, k. As shown, performance drops for large k due to the inclusion of
dissimilar segments, while small k limits training diversity. We find that k = 50 strikes a balance between maintaining
training diversity and avoiding excessive noise from dissimilar segments. These results highlight the importance of
selecting context-target pairs with meaningful observation action similarity for effective Meta-ICL.</p>
                </div>
              </div>
            </div>

            <h3 class="title is-4" style="margin-top: 1.5em;">How does scaling dataset size impact MimicDroid’s ability to perform ICL?</h3>
            
            <div class="columns is-vcentered grey-bg">
              <div class="column is-half">
                <img src="./static/images/success_vs_frames-1.png" alt="Success vs Frames" style="max-width: 600px; width: 100%;" />
              </div>
              <div class="column is-half">
                <div class="content">
                  <p>As shown, we observe consistent performance improvements across all generalization
levels (L1–L3) as the amount of training data increases, demonstrating the scalability of learning from RGB play videos. Specifically, L1 improves from 35% at 128k frames
to 59% at 320k, and L2 rises from 21% to 45%, resulting in a +24% absolute gain for both. L3 also improves, from 16% to 27% (+11%), though the gains are less pronounced compared to L1 and L2. These results affirm the benefits of scaling training data, while also motivating a more systematic
study of the factors that influence ICL performance on harder generalization tasks like L3.</p>
                </div>
              </div>
            </div>

            <h3 class="title is-4" style="margin-top: 1.5em;">Error Analysis</h3>
            <div class="columns is-vcentered">
              <div class="column is-half">
                <img src="./static/images/dual_pie-1.png" alt="Error Analysis" style="max-width: 600px; width: 100%;" />
              </div>
              <div class="column is-half">
                <div class="content">
                  <p><strong>Failure Analysis:</strong> Failures in downstream tasks arise from task misidentification (26%), missed grasps (16%), and
other errors (8%) like incomplete cabinet closure, missed placement (Fig. 4). Compared to Vid2Robot, MIMICDROID notably reduces both misidentification (−15%) and grasping errors (−5%) using ICL.</p>
                </div>
              </div>
            </div>

        </div>
      </div>
      </div>
    </div>
    </section>

            <!-- <h3 class="title is-4" style="margin-top: 1.5em;">GR1 Embodiment</h3>
            <figure class="image">
              <img src="./static/images/real_world_results.png" alt="Real-world experiment results" />
              <figcaption>
                Real-world evaluation on six manipulation tasks using the DROID dataset. For each task, we use only <strong>5 target demonstrations</strong> and retrieve from a pool of 30k successful episodes. COLLAGE achieves an average success rate of 6.83/15, representing a <strong>58% relative performance improvement over STRAP (4.33/15) </strong> and a <strong> 64% improvement over LANG (4.16/15)</strong>. Policies trained solely on the 5 in-domain demonstrations (no retrieval) achieve only 1.00/15 success on average. In contrast, COLLAGE effectively leverages relevant demonstrations from DROID to significantly boost policy performance.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>

    <!-- Rollout Carousel -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Policy Rollouts using MimicDroid</h2>
      <figure>
      <div class="glide" style="position: relative;">
        <div class="glide__track" data-glide-el="track">
          <ul class="glide__slides">
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/potatoes_basket.mp4" type="video/mp4" /></video></div><p class="video-caption">Potatoes-Basket</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_oven_plate_chips.mp4" type="video/mp4" /></video></div><p class="video-caption">Chips-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_rotating_disk.mp4" type="video/mp4" /></video></div><p class="video-caption">Cookies-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_rotating_disk_plate.mp4" type="video/mp4" /></video></div><p class="video-caption">Bread-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/strawberries_to_plate.mp4" type="video/mp4" /></video></div><p class="video-caption">Strawberries-Oven</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/bread_on_oven_tray.mp4" type="video/mp4" /></video></div><p class="video-caption">Bread-Tray</p></div></li>
          </ul>
        </div>
        <div class="glide__arrows" data-glide-el="controls">
          <button class="glide__arrow glide__arrow--left" data-glide-dir="<">‹</button>
          <button class="glide__arrow glide__arrow--right" data-glide-dir=">">›</button>
        </div>
        <div class="glide__bullets" data-glide-el="controls[nav]">
          <button class="glide__bullet" data-glide-dir="=0"></button>
          <button class="glide__bullet" data-glide-dir="=1"></button>
          <button class="glide__bullet" data-glide-dir="=2"></button>
          <button class="glide__bullet" data-glide-dir="=3"></button>
          <button class="glide__bullet" data-glide-dir="=4"></button>
          <button class="glide__bullet" data-glide-dir="=5"></button>
        </div>
      </div>
      <!-- <figcaption class="has-text-centered" style="margin-top: 1rem;">.</figcaption> -->
        
    </figure>
    </div>
  </section> 

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{shah2025mimicdroid,
  title={MimicDroid: In-Context Learning for Humanoid Manipulation by Watching Humans},
  author={Shah, Rutav and Wang, Qi and Jiang, Zhenyu and Liu, Shuijing and Kumar, Sateesh and Seo, Mingyo and Mart{\'\i}n-Mart{\'\i}n, Roberto and Zhu, Yuke},
  journal={arXiv preprint arXiv:},
  year={2025}
}</code></pre>
    </div>
</section>
  <!-- SCRIPTS -->
  <script>
    const panels = Array.from(document.querySelectorAll('.task-panel'));
    let current = 0;
    function showPanel(i) {
      panels[current].style.display = 'none';
      current = (i + panels.length) % panels.length;
      panels[current].style.display = 'block';
    }
    document.querySelectorAll('#prev-btn').forEach(btn => btn.onclick = () => showPanel(current - 1));
    document.querySelectorAll('#next-btn').forEach(btn => btn.onclick = () => showPanel(current + 1));

    // Initialize
    panels.forEach((p, i) => p.style.display = i === 0 ? 'block' : 'none');
  </script>


<!-- <section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Why use multiple feature modalities for retrieval?</h2>
    <p class="subtitle is-6">
      Below, we illustrate the benefits of leveraging on multiple modalities for data retrieval and using COLLAGE to fuse them.
    </p> 

    <!-- Book Place task -->
<!--    <p class="subtitle is-6">
      Retrieved data for the task of <strong>Place the Book in the Back Compartment of the Caddy</strong>.
    </p>
    <figure class="image" style="margin-bottom: 2rem;">
      <img
        src="./static/images/book_caddy_4feats.png"
        alt="Retrieved data for Book Place task"
        style="max-width:100%;"
      />
      <figcaption>
        <!-- Although the instruction <strong>“Place the Book in the Back Compartment of the Caddy”</strong> matches exactly, visual-only retrieval from the prior dataset misses these segments due to appearance differences.
        For this task, the target demonstration involves placing the book in the back compartment of a caddy. The prior dataset contains two helpful types of segments: (1) those with the same scene but a different end-pose, where the book is placed in the front compartment—these are reliably retrieved using visual similarity; and (2) those with the same task but different scene appearance, where the book is correctly placed in the back compartment but the visual context differs—these are primarily retrieved using motion and language cues. Since each modality recovers complementary subsets of relevant data, fusing them in COLLAGE raises accuracy from 66.00 % (visual only) to 89.33 %, yielding an absolute gain of 23.33 percentage points.
    </figcaption>
    </figure> 

    <!-- Cheese Butter task 
    <p class="subtitle is-6">
      Retrieved data for the task of <strong>Put Both the Cream Cheese Box and the Butter in the Basket</strong>.
    </p>
    <figure class="image">
      <img
        src="./static/images/cheese_butter_4feats.png"
        alt="Retrieved data for Cheese Butter task"
        style="max-width:100%;"
      />
      <figcaption>
        In the Cheese Butter task, shape-based retrieval from the prior dataset retrieves far more examples from “Pick up Butter..” and “Pick up Tomato Sauce..” than visual- or motion-based retrieval. These segments provide direct, task-relevant demonstrations of placing similarly shaped objects into the basket. Consequently, the shape modality outperforms both visual and motion, and COLLAGE assigns it the highest weight—yielding performance on par with shape-only retrieval.
      </figcaption>
    </figure>
  </div>
</section> -->


  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          This website is based on the <a href="https://nerfies.github.io/">Nerfies</a> website template,
          licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </footer>

  <!-- Glide JS -->
  <script src="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/glide.min.js"></script>
  <script>
    window.addEventListener('load', function () {
      new Glide('.glide', {
        type: 'carousel',
        rewind: false,
        perView: 3,
        focusAt: 'center',
        gap: 24,
        breakpoints: {
          1024: { perView: 2 },
          768: { perView: 1 }
        }
      }).mount();
    });
  </script>
</body>
</html>
