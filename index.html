<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="" />
  <title>MimicDroid</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/css/glide.core.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">

</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            
            <h1 class="title is-1 publication-title">MimicDroid: </h1>
            <h2 class="subtitle is-2 publication-subtitle">In-Context Learning for Humanoid Manipulation by Watching Humans</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://shahrutav.github.io/" target="_blank">Rutav Shah</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/harry-wang-16470a211/" target="_blank">Qi Wang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://zhenyujiang.me/" target="_blank">Zhenyu Jiang</a><sup>*</sup>,</span>
                    <span class="author-block">
                    <a href="https://shuijing725.github.io/" target="_blank">Shuijing Liu</a><sup>*</sup>,</span>
                    <span class="author-block">
                  <a href="https://sateeshkumar21.github.io/" target="_blank">Sateesh Kumar</a>,</span>
                  <span class="author-block">
                    <a href="https://mingyoseo.com/" target="_blank"> Mingyo Seo</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://robertomartinmartin.com/" target="_blank">Roberto Martín-Martín</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://yukezhu.me/" target="_blank">Yuke Zhu</a>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">

                    <span class="author-block">The University of Texas at Austin
                      <!-- <br>Conferance name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>
                  <br>
                    <div class="column has-text-centered">
                      <div class="publication-links">
                        <!-- PDF Link. -->
                        <span class="link-block">
                          <a href="."
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>Paper</span>
                          </a>
                        </span>
                        <!--arXiv Link.-->
                        <span class="link-block">
                          <a
                            href="."
                            class="external-link button is-normal is-rounded is-dark"
                          >
                            <span class="icon">
                              <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                          </a>
                        </span>
                        <!-- Video Link. -->
                        <!-- <span class="link-block">
                          <a href="https://www.youtube.com/watch?v=ts0e2HHp3Lk"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-youtube"></i>
                            </span>
                            <span>Video</span>
                          </a>
                        </span> -->
                        <!-- Code Link. -->
                        <span class="link-block">
                          <a href="."
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code (Coming Soon)</span>
                            </a>
                        </span>
                        <!-- Dataset Link. -->
                        <!-- <span class="link-block">
                          <a href="https://drive.google.com/drive/folders/1mNJmnyzIoCudRcTdRVrN3WAiuWIM8355"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="far fa-images"></i>
                            </span>
                            <span>Dataset</span>
                            </a> -->
                      </div>
                    </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract 
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">    
    <div class="column is-four-fifths">
  <!-- <div class="content " style="background:#f0f0f0; padding:20px 24px; border:1px solid #ccc; border-radius:6px;">
  <p style="margin:0;">
     <strong>TL;DR:</strong> We present <strong>COLLAGE</strong>, a few-shot imitation learning method that fuses data subsets retrieved from large-scale datasets using multiple measures of similarity to the target task. It scores each subset by training an imitation learning policy on it, measuring how likely it is to produce the target actions, and uses these scores to prioritize the most effective data during training.
  </p>
</div>
    </div>
    </div>
    </div>
    </div>
  </div>-->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
     
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">  
            <p>
              We envision humanoids that can efficiently adapt to new tasks, achieving few-shot manipulation in ever-changing environments. In-context learning (ICL) is a promising framework to achieve this goal, but existing ICL approaches typically rely on costly teleoperated datasets or limited expressivity of simulations for training. In this work, we propose using human play videos—continuous, unlabeled videos of people interacting freely with their environment by their curiosity—as a scalable and diverse data source. Learning from human play videos poses two key challenges: (i) the absence of labeled data to train policies to learn in-context; (ii) the embodiment gap between humans and humanoids. We introduce MimicDroid, a method that enables humanoids to perform ICL by using only human play videos as training data. To overcome the lack of supervision, MimicDroid extracts trajectory pairs exhibiting similar manipulation behaviors in a self-supervised manner, e.g., “placing the bread on the oven tray,” and “placing a bagel on the oven tray.” The policy is trained to predict actions of one trajectory, conditioned on others with similar manipulation behaviors. This trains the model to learn in-context from the provided examples. To address the embodiment gap, first, MimicDroid retargets human wrist poses estimated from RGB videos to the humanoid, leveraging their kinematic similarity. Second, to handle the visual gap, it applies random patch masking during training to prevent overfitting to human-specific cues. To systematically evaluate few-shot learning for humanoids, we introduce an open-source simulation benchmark with three levels of increasing generalization difficulty from seen to unseen objects and environments. In both simulation and the real world, MimicDroid outperforms state-of-the-art methods, achieving nearly twofold better generalization to novel objects in the real world.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


    <!-- Rollout Carousel -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Policy Rollouts with MimicDroid</h2>
      <figure>
      <div class="glide" style="position: relative;">
        <div class="glide__track" data-glide-el="track">
          <ul class="glide__slides">
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/potatoes_basket.mp4" type="video/mp4" /></video></div><p class="video-caption">Potatoes-Basket</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_oven_plate_chips.mp4" type="video/mp4" /></video></div><p class="video-caption">Chips-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_rotating_disk.mp4" type="video/mp4" /></video></div><p class="video-caption">Cookies-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_rotating_disk_plate.mp4" type="video/mp4" /></video></div><p class="video-caption">Bread-Plate</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/strawberries_to_plate.mp4" type="video/mp4" /></video></div><p class="video-caption">Strawberries-Oven</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/bread_on_oven_tray.mp4" type="video/mp4" /></video></div><p class="video-caption">Bread-Tray</p></div></li>
          </ul>
        </div>
        <div class="glide__arrows" data-glide-el="controls">
          <button class="glide__arrow glide__arrow--left" data-glide-dir="<">‹</button>
          <button class="glide__arrow glide__arrow--right" data-glide-dir=">">›</button>
        </div>
        <div class="glide__bullets" data-glide-el="controls[nav]">
          <button class="glide__bullet" data-glide-dir="=0"></button>
          <button class="glide__bullet" data-glide-dir="=1"></button>
          <button class="glide__bullet" data-glide-dir="=2"></button>
          <button class="glide__bullet" data-glide-dir="=3"></button>
          <button class="glide__bullet" data-glide-dir="=4"></button>
          <button class="glide__bullet" data-glide-dir="=5"></button>
        </div>
      </div>
      <!-- <figcaption class="has-text-centered" style="margin-top: 1rem;">.</figcaption> -->
        
    </figure>
    </div>
  </section> 


  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Video</h2>
      <p class="subtitle is-6">
        Watch the video to see MimicDroid in action!
      </p>

      <video
        controls
        preload="metadata"
        style="max-width:100%; border-radius:8px;"
      >
        <source src="./static/images/summary_video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Method Overview</h2>
    <figure class="image">
      <img src="./static/images/mew_method_figurev2-1.png" 
           alt="Overview of MIMICDROID" />
          
      <figcaption class="method-caption has-text-justified">
        <strong>Overview of MimicDroid.</strong> MimicDroid performs meta-training for in-context learning (Meta-ICL) by constructing context-target pairs from human play videos.
    For a target segment, we retrieve the top-k most similar trajectory segments (top-left) based on observation-action similarity (top-right) to serve as context.
    These context-target pairs are used to teach the policy in-context learning (bottom-left). To overcome the human-robot visual gap and avoid overfitting to human-specific visual cues, we apply visual masking to input images (bottom-right), improving transferability.
      </figcaption>
    </figure>
  </div>
</section>

  

  <!-- Experiments -->
   <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Experiments</h2>
          <div class="content">
            <figure class="image">
              <img src="./static/images/results.png" alt="experiment results" />
              <figcaption>
               <p>Success rates in Abstract and GR1 embodiments in the simulation benchmark</p>
              </figcaption>
            </figure>
            <h3 class="title is-4" style="margin-top: 1.5em;">Quantitative Results</h3>
            
            <div class="columns is-multiline is-centered">
              <div class="column is-one-third">
                <figure class="image">
                  <img src="./static/images/n_prompts-1.png" alt="Number of Prompts Results" />
                  <figcaption>Effect of number of prompts on performance.</figcaption>
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image">
                  <img src="./static/images/n_topk-1.png" alt="Top-K Results" />
                  <figcaption>Effect of top-K retrieval on performance.</figcaption>
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image">
                  <img src="./static/images/success_vs_frames-1.png" alt="Success vs Frames" />
                  <figcaption>Success rate as a function of frames.</figcaption>
                </figure>
              </div>
            </div>

            <h3 class="title is-4" style="margin-top: 1.5em;">Qualitative Analysis</h3>
            <figure class="image">
              <img src="./static/images/mew_rw_figure-1.png" alt="Qualitative Method Overview" />
              <figcaption>Qualitative overview of MimicDroid method.</figcaption>
            </figure>

        </div>
      </div>
      </div>
    </div>
    </section>

            <!-- <h3 class="title is-4" style="margin-top: 1.5em;">GR1 Embodiment</h3>
            <figure class="image">
              <img src="./static/images/real_world_results.png" alt="Real-world experiment results" />
              <figcaption>
                Real-world evaluation on six manipulation tasks using the DROID dataset. For each task, we use only <strong>5 target demonstrations</strong> and retrieve from a pool of 30k successful episodes. COLLAGE achieves an average success rate of 6.83/15, representing a <strong>58% relative performance improvement over STRAP (4.33/15) </strong> and a <strong> 64% improvement over LANG (4.16/15)</strong>. Policies trained solely on the 5 in-domain demonstrations (no retrieval) achieve only 1.00/15 success on average. In contrast, COLLAGE effectively leverages relevant demonstrations from DROID to significantly boost policy performance.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div> -->
  </section>
    <!-- Weights Predicted by COLLAGE -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Importance Weights Predicted by COLLAGE</h2>
      <div class="has-text-centered">
        <figure class="image" style="max-width: 950px; width: 100%; margin: 0 auto;">
          <img src="./static/images/modality_weights_pie_chart.png" alt="Modality Weights Pie Chart" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption style="margin-top: 0.75rem; font-size: 1rem; color: #444;">
            <figcaption style="margin-top: 0.75rem;">
                Importance weights assigned by <strong>COLLAGE</strong> to different modalities used in our framework (Visual, Motion, Shape, Language).              
          </figcaption>
        </figure>
      </div>
    </div>
  </section> -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{shah2025mimicdroid,
      title={MimicDroid: In-Context Learning for Humanoid Manipulation by Watching Humans},
      author={Shah, Rutav and Wang, Qi and Jiang, Zhenyu and Liu, Shuijing and Kumar, Sateesh and Seo, Mingyo and Mart{\'\i}n-Mart{\'\i}n, Roberto and Zhu, Yuke},
      journal={arXiv preprint arXiv:},
      year={2025}
    }
      </code></pre>
    </div>
</section>
  <!-- SCRIPTS -->
  <script>
    const panels = Array.from(document.querySelectorAll('.task-panel'));
    let current = 0;
    function showPanel(i) {
      panels[current].style.display = 'none';
      current = (i + panels.length) % panels.length;
      panels[current].style.display = 'block';
    }
    document.querySelectorAll('#prev-btn').forEach(btn => btn.onclick = () => showPanel(current - 1));
    document.querySelectorAll('#next-btn').forEach(btn => btn.onclick = () => showPanel(current + 1));

    // Initialize
    panels.forEach((p, i) => p.style.display = i === 0 ? 'block' : 'none');
  </script>


<!-- <section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Why use multiple feature modalities for retrieval?</h2>
    <p class="subtitle is-6">
      Below, we illustrate the benefits of leveraging on multiple modalities for data retrieval and using COLLAGE to fuse them.
    </p> 

    <!-- Book Place task -->
<!--    <p class="subtitle is-6">
      Retrieved data for the task of <strong>Place the Book in the Back Compartment of the Caddy</strong>.
    </p>
    <figure class="image" style="margin-bottom: 2rem;">
      <img
        src="./static/images/book_caddy_4feats.png"
        alt="Retrieved data for Book Place task"
        style="max-width:100%;"
      />
      <figcaption>
        <!-- Although the instruction <strong>“Place the Book in the Back Compartment of the Caddy”</strong> matches exactly, visual-only retrieval from the prior dataset misses these segments due to appearance differences.
        For this task, the target demonstration involves placing the book in the back compartment of a caddy. The prior dataset contains two helpful types of segments: (1) those with the same scene but a different end-pose, where the book is placed in the front compartment—these are reliably retrieved using visual similarity; and (2) those with the same task but different scene appearance, where the book is correctly placed in the back compartment but the visual context differs—these are primarily retrieved using motion and language cues. Since each modality recovers complementary subsets of relevant data, fusing them in COLLAGE raises accuracy from 66.00 % (visual only) to 89.33 %, yielding an absolute gain of 23.33 percentage points.
    </figcaption>
    </figure> 

    <!-- Cheese Butter task 
    <p class="subtitle is-6">
      Retrieved data for the task of <strong>Put Both the Cream Cheese Box and the Butter in the Basket</strong>.
    </p>
    <figure class="image">
      <img
        src="./static/images/cheese_butter_4feats.png"
        alt="Retrieved data for Cheese Butter task"
        style="max-width:100%;"
      />
      <figcaption>
        In the Cheese Butter task, shape-based retrieval from the prior dataset retrieves far more examples from “Pick up Butter..” and “Pick up Tomato Sauce..” than visual- or motion-based retrieval. These segments provide direct, task-relevant demonstrations of placing similarly shaped objects into the basket. Consequently, the shape modality outperforms both visual and motion, and COLLAGE assigns it the highest weight—yielding performance on par with shape-only retrieval.
      </figcaption>
    </figure>
  </div>
</section> -->


  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          This website is based on the <a href="https://nerfies.github.io/">Nerfies</a> website template,
          licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </footer>

  <!-- Glide JS -->
  <script src="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/glide.min.js"></script>
  <script>
    window.addEventListener('load', function () {
      new Glide('.glide', {
        type: 'carousel',
        rewind: false,
        perView: 3,
        focusAt: 'center',
        gap: 24,
        breakpoints: {
          1024: { perView: 2 },
          768: { perView: 1 }
        }
      }).mount();
    });
  </script>
</body>
</html>
